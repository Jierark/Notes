# Society
What is society? One definition is that it is a collection of life (dynamic material, energy, and information). Some philosophers, such as Locke, Hobbes, and Aristotle, believed that society is supposed to help us reach our natural desires, and avoid our natural fears more easily. Society is not fully controlled by any one member, meaning each person has their own sets of desires and fears for it.

The ability to participate in society freely, however, requires **Information**. Formally, information is structured difference and differentiation - patterns of energy or space that can be recognized, transmitted, and/or transformed. Humans understand and remember it as stories, both good and bad, or relate it to experience. This shapes a person and the decisions they tend to make.

Sometimes, society restricts the information people can see due to safety and fairness. A few examples:
- 18 U.S. Code 2252 - criminalizes knowing receipt, distribution, or possession of child sexual abuse material
- Fairness Doctrine - requires broadcasters to present controversial issues of public importance and to provide contrasting viewpoints

How do you stake a claim on your own information? Who actually owns it?
Let's think about it this way: Information is capital.
Capital is something that helps you to do work. Work information produces aggregate information which has economic, social, and political value. Capital can be owned traditionally, so how does one stake claim on their own capital? Typically we have taxes and rents on capital exchanges when they serve the public good. Does this mean we can apply the same ideas to information? Well, we have social institutions that should be able to make the decisions on our behalf.

Ok, this was a ton of random philosophy for an AI ethics course. Hopefully it wasn't completely boring. This is to lead us to the central idea of AI Janus:
- With AI, our hope is that the informational insights through centralization will help us solve our biggest problems.
- But there is a very present fear that the informational insights through centralization are dangerous and will destroy us.
There's an interesting parallel of this to the Panopticon, which is a specific design for a prison, seen in different lights by the Bentham brothers. While one (Jeremy) saw it in a utilitarianism light to reform social institutions, the other (Samuel) saw it in a different lens - being coined as the father of prison designs.

AI, like a panopticon, is a tool. It serves a means to an end, but does not prescribe which ends to pursue. You can ask similar questions to both: do you trust it to be working on your behalf? Do you know how it works?

Panopticons have a real impact on how people behave. Your compliance with institutional goals is only guaranteed if you fear the possibility of deviating from them. This brings us to the Alignment Problem:

If your desires are "misaligned" with the decider's desires, then you can be made (infinitely) worse-off if the decider is given the power to (over-)optimize.

We can frame this in many different ways, but let's try framing it in terms of AI:

If society's desires are "misaligned" with the AI's desires, then society can be made (infinitely) worse-off if the AI is given the power to (over)optimize.

These concerns are an interesting point of discussion and similar.

Building and training AI to satisfy our needs turns out to be a very difficult problem. It may not be obvious initially, but hidden biases plague much of the world's data, and it only becomes evident inside these models.

This class will explore many of these examples and discuss what we can do to address these concerns.